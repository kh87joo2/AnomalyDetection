{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import getpass\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "REPO_DIR = Path('/content/AnomalyDetection')\n",
    "REPO_URL = os.environ.get('ANOMALY_REPO_URL', 'https://github.com/kh87joo2/AnomalyDetection.git')\n",
    "FORCE_RECLONE = os.environ.get('ANOMALY_FORCE_RECLONE', '0') == '1'\n",
    "\n",
    "\n",
    "def is_repo_root(path: Path) -> bool:\n",
    "    required = [\n",
    "        path / 'requirements.txt',\n",
    "        path / 'configs',\n",
    "        path / 'trainers',\n",
    "        path / 'configs' / 'patchtst_ssl.yaml',\n",
    "        path / 'configs' / 'swinmae_ssl.yaml',\n",
    "    ]\n",
    "    return all(p.exists() for p in required)\n",
    "\n",
    "\n",
    "def clone_repo(repo_url: str, token: str = '') -> subprocess.CompletedProcess:\n",
    "    clone_url = repo_url\n",
    "    if token and repo_url.startswith('https://'):\n",
    "        clone_url = repo_url.replace('https://', f'https://{token}@', 1)\n",
    "    return subprocess.run(['git', 'clone', clone_url, str(REPO_DIR)], text=True, capture_output=True)\n",
    "\n",
    "\n",
    "if is_repo_root(REPO_DIR):\n",
    "    print(f'[info] using existing repo: {REPO_DIR}')\n",
    "elif REPO_DIR.exists():\n",
    "    if FORCE_RECLONE:\n",
    "        print(f'[warn] removing existing directory because ANOMALY_FORCE_RECLONE=1: {REPO_DIR}')\n",
    "        shutil.rmtree(REPO_DIR)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f'{REPO_DIR} exists but is not a valid repo root.\\n'\n",
    "            'Set ANOMALY_FORCE_RECLONE=1 to allow removal and reclone, or fix the directory manually.'\n",
    "        )\n",
    "\n",
    "if not is_repo_root(REPO_DIR):\n",
    "    print(f'[info] cloning repo: {REPO_URL}')\n",
    "    token = os.environ.get('ANOMALY_GH_TOKEN', '').strip()\n",
    "    result = clone_repo(REPO_URL, token=token)\n",
    "\n",
    "    if result.returncode != 0 and not token:\n",
    "        print('[warn] public clone failed. If repo is private, enter a GitHub PAT.')\n",
    "        token = getpass.getpass('GitHub PAT (private repo only): ').strip()\n",
    "        if token:\n",
    "            result = clone_repo(REPO_URL, token=token)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        if result.stderr:\n",
    "            print('[git]', result.stderr.strip().splitlines()[-1])\n",
    "        raise RuntimeError('git clone failed. Check repo URL, network, and token permissions.')\n",
    "\n",
    "if not is_repo_root(REPO_DIR):\n",
    "    raise FileNotFoundError('Repo root validation failed after clone.')\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print('cwd:', Path.cwd())\n",
    "print('requirements.txt exists:', Path('requirements.txt').exists())\n",
    "print('configs exists:', Path('configs').exists())\n",
    "print('trainers exists:', Path('trainers').exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAGGLE_DATA_DOWNLOAD_PATCHTST\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "REPO_DIR = Path('/content/AnomalyDetection')\n",
    "RAW_DIR = REPO_DIR / 'data' / 'raw' / 'swat'\n",
    "OUT_DIR = REPO_DIR / 'data' / 'fdc'\n",
    "DATASET = 'vishala28/swat-dataset-secure-water-treatment-system'\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def run(cmd):\n",
    "    cmd = [str(x) for x in cmd]\n",
    "    print('$', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "\n",
    "run([sys.executable, '-m', 'pip', 'install', '-q', 'kaggle', 'pandas', 'pyyaml'])\n",
    "\n",
    "kaggle_dir = Path('/root/.kaggle')\n",
    "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "kaggle_json = kaggle_dir / 'kaggle.json'\n",
    "\n",
    "if not kaggle_json.exists():\n",
    "    user = os.environ.get('KAGGLE_USERNAME', '').strip()\n",
    "    key = os.environ.get('KAGGLE_KEY', '').strip()\n",
    "    if user and key:\n",
    "        kaggle_json.write_text(json.dumps({'username': user, 'key': key}), encoding='utf-8')\n",
    "    else:\n",
    "        from google.colab import files\n",
    "\n",
    "        print('Upload kaggle.json from https://www.kaggle.com/settings/account')\n",
    "        uploaded = files.upload()\n",
    "        if 'kaggle.json' not in uploaded:\n",
    "            raise FileNotFoundError('kaggle.json not uploaded')\n",
    "        with kaggle_json.open('wb') as f:\n",
    "            f.write(uploaded['kaggle.json'])\n",
    "\n",
    "os.chmod(kaggle_json, 0o600)\n",
    "\n",
    "run(['kaggle', 'datasets', 'download', '-d', DATASET, '-p', str(RAW_DIR), '--force'])\n",
    "\n",
    "for zip_path in sorted(RAW_DIR.glob('*.zip')):\n",
    "    print('[extract]', zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(RAW_DIR)\n",
    "\n",
    "csv_files = sorted(RAW_DIR.rglob('*.csv'))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f'No CSV found under {RAW_DIR}')\n",
    "\n",
    "print('csv_found:', len(csv_files))\n",
    "for p in csv_files[:20]:\n",
    "    print('-', p)\n",
    "\n",
    "\n",
    "def pick_fdc_csv(files):\n",
    "    def score(path: Path):\n",
    "        name = path.name.lower()\n",
    "        if 'normal' in name:\n",
    "            return (0, len(name))\n",
    "        if 'train' in name:\n",
    "            return (1, len(name))\n",
    "        if 'attack' in name:\n",
    "            return (9, len(name))\n",
    "        return (5, len(name))\n",
    "\n",
    "    return sorted(files, key=score)[0]\n",
    "\n",
    "\n",
    "src = pick_fdc_csv(csv_files)\n",
    "print('selected_source:', src)\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "orig_cols = list(df.columns)\n",
    "\n",
    "col_map = {str(c).strip().lower(): c for c in df.columns}\n",
    "ts_col = None\n",
    "for key in ['timestamp', 'time', 'datetime', 'date']:\n",
    "    if key in col_map:\n",
    "        ts_col = col_map[key]\n",
    "        break\n",
    "\n",
    "if ts_col is None:\n",
    "    df.insert(0, 'timestamp', range(len(df)))\n",
    "else:\n",
    "    df = df.rename(columns={ts_col: 'timestamp'})\n",
    "\n",
    "# Drop common label columns if present.\n",
    "drop_names = {'label', 'attack', 'normal/attack', 'normal_attack', 'class', 'status'}\n",
    "for c in list(df.columns):\n",
    "    if str(c).strip().lower() in drop_names:\n",
    "        df = df.drop(columns=[c])\n",
    "\n",
    "# Numeric-cast all non-timestamp columns.\n",
    "feature_cols = [c for c in df.columns if c != 'timestamp']\n",
    "for c in feature_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "out_csv = OUT_DIR / 'swat_fdc_train.csv'\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "print('prepared_csv:', out_csv)\n",
    "print('shape:', df.shape)\n",
    "print('columns_in:', len(orig_cols), 'columns_out:', len(df.columns))\n",
    "\n",
    "base_cfg = REPO_DIR / 'configs' / 'patchtst_ssl.yaml'\n",
    "real_cfg = REPO_DIR / 'configs' / 'patchtst_ssl_real.yaml'\n",
    "cfg = yaml.safe_load(base_cfg.read_text(encoding='utf-8'))\n",
    "cfg['data']['source'] = 'csv'\n",
    "cfg['data']['path'] = '/content/AnomalyDetection/data/fdc/*.csv'\n",
    "cfg['data']['timestamp_col'] = 'timestamp'\n",
    "real_cfg.write_text(yaml.safe_dump(cfg, sort_keys=False), encoding='utf-8')\n",
    "\n",
    "print('real_config_written:', real_cfg)\n",
    "print('now run the training cell below; it uses *_real.yaml if present')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_CHECK_PATCHTST\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "fdc_dir = Path('/content/AnomalyDetection/data/fdc')\n",
    "csv_files = sorted(fdc_dir.glob('*.csv'))\n",
    "\n",
    "print('fdc_dir:', fdc_dir)\n",
    "print('csv_count:', len(csv_files))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f'No CSV files found in {fdc_dir}. Run the Kaggle download cell first.')\n",
    "\n",
    "sample = csv_files[0]\n",
    "print('sample_file:', sample)\n",
    "\n",
    "df = pd.read_csv(sample)\n",
    "print('shape:', df.shape)\n",
    "print('columns[:20]:', df.columns[:20].tolist())\n",
    "print(df.head(3))\n",
    "\n",
    "required = {'timestamp'}\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    print('[warn] missing required columns:', missing)\n",
    "else:\n",
    "    print('[ok] required columns exist')\n",
    "\n",
    "non_ts_cols = [c for c in df.columns if c != 'timestamp']\n",
    "if non_ts_cols:\n",
    "    numeric_na_ratio = df[non_ts_cols].apply(pd.to_numeric, errors='coerce').isna().mean().mean()\n",
    "    print('non_timestamp_numeric_na_ratio:', float(numeric_na_ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import shlex\n",
    "\n",
    "\n",
    "def run(cmd):\n",
    "    cmd = [str(x) for x in cmd]\n",
    "    print('$', ' '.join(shlex.quote(x) for x in cmd))\n",
    "    proc = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "    )\n",
    "\n",
    "    assert proc.stdout is not None\n",
    "    for line in proc.stdout:\n",
    "        print(line, end='')\n",
    "\n",
    "    code = proc.wait()\n",
    "    if code != 0:\n",
    "        raise RuntimeError(f\"Command failed ({code}): {' '.join(cmd)}\")\n",
    "\n",
    "\n",
    "req = Path('requirements.txt')\n",
    "if not req.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"requirements.txt not found in cwd={Path.cwd()}. Run bootstrap cell first or fix repo path.\"\n",
    "    )\n",
    "\n",
    "run([sys.executable, '-m', 'pip', 'install', '-U', 'pip'])\n",
    "run([sys.executable, '-m', 'pip', 'install', '-r', str(req)])\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print('torch:', torch.__version__)\n",
    "print('cuda_available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda_device_count:', torch.cuda.device_count())\n",
    "    print('cuda_device_0:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "cfg = Path('configs/patchtst_ssl_real.yaml')\n",
    "if not cfg.exists():\n",
    "    print('[warn] real config not found, fallback to synthetic config')\n",
    "    cfg = Path('configs/patchtst_ssl.yaml')\n",
    "\n",
    "cmd = [sys.executable, '-m', 'trainers.train_patchtst_ssl', '--config', str(cfg)]\n",
    "print('$', ' '.join(cmd))\n",
    "result = subprocess.run(cmd)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(f\"Training failed with exit code {result.returncode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = Path('checkpoints/patchtst_ssl.pt')\n",
    "print('checkpoint_exists:', checkpoint_path.exists(), checkpoint_path)\n",
    "assert checkpoint_path.exists(), f'Missing checkpoint: {checkpoint_path}'\n",
    "print('checkpoint_size_bytes:', checkpoint_path.stat().st_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}