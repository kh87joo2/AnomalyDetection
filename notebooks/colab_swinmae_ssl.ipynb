{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import getpass\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "REPO_DIR = Path('/content/AnomalyDetection')\n",
    "REPO_URL = os.environ.get('ANOMALY_REPO_URL', 'https://github.com/kh87joo2/AnomalyDetection.git')\n",
    "FORCE_RECLONE = os.environ.get('ANOMALY_FORCE_RECLONE', '0') == '1'\n",
    "\n",
    "\n",
    "def is_repo_root(path: Path) -> bool:\n",
    "    required = [\n",
    "        path / 'requirements.txt',\n",
    "        path / 'configs',\n",
    "        path / 'trainers',\n",
    "        path / 'configs' / 'patchtst_ssl.yaml',\n",
    "        path / 'configs' / 'swinmae_ssl.yaml',\n",
    "    ]\n",
    "    return all(p.exists() for p in required)\n",
    "\n",
    "\n",
    "def clone_repo(repo_url: str, token: str = '') -> subprocess.CompletedProcess:\n",
    "    clone_url = repo_url\n",
    "    if token and repo_url.startswith('https://'):\n",
    "        clone_url = repo_url.replace('https://', f'https://{token}@', 1)\n",
    "    return subprocess.run(['git', 'clone', clone_url, str(REPO_DIR)], text=True, capture_output=True)\n",
    "\n",
    "\n",
    "if is_repo_root(REPO_DIR):\n",
    "    print(f'[info] using existing repo: {REPO_DIR}')\n",
    "elif REPO_DIR.exists():\n",
    "    if FORCE_RECLONE:\n",
    "        print(f'[warn] removing existing directory because ANOMALY_FORCE_RECLONE=1: {REPO_DIR}')\n",
    "        shutil.rmtree(REPO_DIR)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f'{REPO_DIR} exists but is not a valid repo root.\\n'\n",
    "            'Set ANOMALY_FORCE_RECLONE=1 to allow removal and reclone, or fix the directory manually.'\n",
    "        )\n",
    "\n",
    "if not is_repo_root(REPO_DIR):\n",
    "    print(f'[info] cloning repo: {REPO_URL}')\n",
    "    token = os.environ.get('ANOMALY_GH_TOKEN', '').strip()\n",
    "    result = clone_repo(REPO_URL, token=token)\n",
    "\n",
    "    if result.returncode != 0 and not token:\n",
    "        print('[warn] public clone failed. If repo is private, enter a GitHub PAT.')\n",
    "        token = getpass.getpass('GitHub PAT (private repo only): ').strip()\n",
    "        if token:\n",
    "            result = clone_repo(REPO_URL, token=token)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        if result.stderr:\n",
    "            print('[git]', result.stderr.strip().splitlines()[-1])\n",
    "        raise RuntimeError('git clone failed. Check repo URL, network, and token permissions.')\n",
    "\n",
    "if not is_repo_root(REPO_DIR):\n",
    "    raise FileNotFoundError('Repo root validation failed after clone.')\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print('cwd:', Path.cwd())\n",
    "print('requirements.txt exists:', Path('requirements.txt').exists())\n",
    "print('configs exists:', Path('configs').exists())\n",
    "print('trainers exists:', Path('trainers').exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAGGLE_DATA_DOWNLOAD_SWINMAE\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "REPO_DIR = Path('/content/AnomalyDetection')\n",
    "RAW_DIR = REPO_DIR / 'data' / 'raw' / 'vibration'\n",
    "OUT_DIR = REPO_DIR / 'data' / 'vib'\n",
    "DATASET = 'mohdsufianbinothman/triaxial-bearing-dataset'\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def run(cmd):\n",
    "    cmd = [str(x) for x in cmd]\n",
    "    print('$', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "\n",
    "run([sys.executable, '-m', 'pip', 'install', '-q', 'kaggle', 'pandas', 'numpy', 'pyyaml'])\n",
    "\n",
    "kaggle_dir = Path('/root/.kaggle')\n",
    "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "kaggle_json = kaggle_dir / 'kaggle.json'\n",
    "\n",
    "if not kaggle_json.exists():\n",
    "    user = os.environ.get('KAGGLE_USERNAME', '').strip()\n",
    "    key = os.environ.get('KAGGLE_KEY', '').strip()\n",
    "    if user and key:\n",
    "        kaggle_json.write_text(json.dumps({'username': user, 'key': key}), encoding='utf-8')\n",
    "    else:\n",
    "        from google.colab import files\n",
    "\n",
    "        print('Upload kaggle.json from https://www.kaggle.com/settings/account')\n",
    "        uploaded = files.upload()\n",
    "        if 'kaggle.json' not in uploaded:\n",
    "            raise FileNotFoundError('kaggle.json not uploaded')\n",
    "        with kaggle_json.open('wb') as f:\n",
    "            f.write(uploaded['kaggle.json'])\n",
    "\n",
    "os.chmod(kaggle_json, 0o600)\n",
    "\n",
    "run(['kaggle', 'datasets', 'download', '-d', DATASET, '-p', str(RAW_DIR), '--force'])\n",
    "\n",
    "for zip_path in sorted(RAW_DIR.glob('*.zip')):\n",
    "    print('[extract]', zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(RAW_DIR)\n",
    "\n",
    "csv_files = sorted(RAW_DIR.rglob('*.csv'))\n",
    "npy_files = sorted(RAW_DIR.rglob('*.npy'))\n",
    "print('csv_found:', len(csv_files), 'npy_found:', len(npy_files))\n",
    "\n",
    "for p in csv_files[:20]:\n",
    "    print('-', p)\n",
    "for p in npy_files[:20]:\n",
    "    print('-', p)\n",
    "\n",
    "\n",
    "def norm(name: str) -> str:\n",
    "    return re.sub(r'[^a-z0-9]+', '_', name.strip().lower()).strip('_')\n",
    "\n",
    "\n",
    "def detect_axis_map(columns):\n",
    "    normalized = {norm(c): c for c in columns}\n",
    "    direct = {'x': None, 'y': None, 'z': None}\n",
    "\n",
    "    for axis in ['x', 'y', 'z']:\n",
    "        if axis in normalized:\n",
    "            direct[axis] = normalized[axis]\n",
    "\n",
    "    if all(direct.values()):\n",
    "        return direct\n",
    "\n",
    "    patterns = {\n",
    "        'x': ['x', 'acc_x', 'axis_x', 'x_axis', 'ax', 'vib_x'],\n",
    "        'y': ['y', 'acc_y', 'axis_y', 'y_axis', 'ay', 'vib_y'],\n",
    "        'z': ['z', 'acc_z', 'axis_z', 'z_axis', 'az', 'vib_z'],\n",
    "    }\n",
    "    out = {'x': None, 'y': None, 'z': None}\n",
    "    for axis, keys in patterns.items():\n",
    "        for k in keys:\n",
    "            if k in normalized:\n",
    "                out[axis] = normalized[k]\n",
    "                break\n",
    "    if all(out.values()):\n",
    "        return out\n",
    "    return None\n",
    "\n",
    "\n",
    "prepared = None\n",
    "for path in csv_files:\n",
    "    try:\n",
    "        head = pd.read_csv(path, nrows=5)\n",
    "    except Exception:\n",
    "        continue\n",
    "    axis_map = detect_axis_map(list(head.columns))\n",
    "    if axis_map is None:\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    x_col = axis_map['x']\n",
    "    y_col = axis_map['y']\n",
    "    z_col = axis_map['z']\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "\n",
    "    col_map = {str(c).strip().lower(): c for c in df.columns}\n",
    "    ts_col = None\n",
    "    for key in ['timestamp', 'time', 'datetime', 'date']:\n",
    "        if key in col_map:\n",
    "            ts_col = col_map[key]\n",
    "            break\n",
    "\n",
    "    if ts_col is None:\n",
    "        out['timestamp'] = range(len(df))\n",
    "    else:\n",
    "        out['timestamp'] = df[ts_col]\n",
    "\n",
    "    out['x'] = pd.to_numeric(df[x_col], errors='coerce')\n",
    "    out['y'] = pd.to_numeric(df[y_col], errors='coerce')\n",
    "    out['z'] = pd.to_numeric(df[z_col], errors='coerce')\n",
    "\n",
    "    out_csv = OUT_DIR / 'bearing_xyz.csv'\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    prepared = out_csv\n",
    "    print('prepared_csv:', out_csv, 'shape:', out.shape)\n",
    "    break\n",
    "\n",
    "if prepared is None:\n",
    "    # Fallback: first npy with shape (T,3)\n",
    "    for path in npy_files:\n",
    "        arr = np.load(path)\n",
    "        if arr.ndim == 2 and arr.shape[1] == 3:\n",
    "            out_npy = OUT_DIR / 'bearing_xyz.npy'\n",
    "            np.save(out_npy, arr.astype(np.float32))\n",
    "            prepared = out_npy\n",
    "            print('prepared_npy:', out_npy, 'shape:', arr.shape)\n",
    "            break\n",
    "\n",
    "if prepared is None:\n",
    "    raise RuntimeError('Could not build standardized vibration input (x,y,z). Check dataset file schema.')\n",
    "\n",
    "base_cfg = REPO_DIR / 'configs' / 'swinmae_ssl.yaml'\n",
    "real_cfg = REPO_DIR / 'configs' / 'swinmae_ssl_real.yaml'\n",
    "cfg = yaml.safe_load(base_cfg.read_text(encoding='utf-8'))\n",
    "\n",
    "if str(prepared).endswith('.npy'):\n",
    "    cfg['data']['source'] = 'npy'\n",
    "    cfg['data']['path'] = '/content/AnomalyDetection/data/vib/*.npy'\n",
    "else:\n",
    "    cfg['data']['source'] = 'csv'\n",
    "    cfg['data']['path'] = '/content/AnomalyDetection/data/vib/*.csv'\n",
    "\n",
    "cfg['data']['timestamp_col'] = 'timestamp'\n",
    "real_cfg.write_text(yaml.safe_dump(cfg, sort_keys=False), encoding='utf-8')\n",
    "\n",
    "print('real_config_written:', real_cfg)\n",
    "print('important: set data.fs in configs/swinmae_ssl_real.yaml to real sampling rate')\n",
    "print('now run the training cell below; it uses *_real.yaml if present')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_CHECK_SWINMAE\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "vib_dir = Path('/content/AnomalyDetection/data/vib')\n",
    "csv_files = sorted(vib_dir.glob('*.csv'))\n",
    "npy_files = sorted(vib_dir.glob('*.npy'))\n",
    "\n",
    "print('vib_dir:', vib_dir)\n",
    "print('csv_count:', len(csv_files))\n",
    "print('npy_count:', len(npy_files))\n",
    "\n",
    "if not csv_files and not npy_files:\n",
    "    raise FileNotFoundError(f'No CSV/NPY files found in {vib_dir}. Run the Kaggle download cell first.')\n",
    "\n",
    "if csv_files:\n",
    "    sample = csv_files[0]\n",
    "    print('sample_csv:', sample)\n",
    "    df = pd.read_csv(sample)\n",
    "    print('shape:', df.shape)\n",
    "    print('columns:', df.columns.tolist())\n",
    "    print(df.head(3))\n",
    "\n",
    "    req = {'x', 'y', 'z'}\n",
    "    miss = [c for c in req if c not in df.columns]\n",
    "    if miss:\n",
    "        print('[warn] missing required axis columns:', miss)\n",
    "    else:\n",
    "        print('[ok] x,y,z columns exist')\n",
    "\n",
    "    if 'timestamp' in df.columns:\n",
    "        ts_dt = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        dt_sec = ts_dt.diff().dt.total_seconds().dropna()\n",
    "        dt_sec = dt_sec[dt_sec > 0]\n",
    "\n",
    "        if len(dt_sec) > 0:\n",
    "            fs_est = 1.0 / float(dt_sec.median())\n",
    "            print('estimated_fs_from_datetime_median_dt:', fs_est)\n",
    "        else:\n",
    "            ts_num = pd.to_numeric(df['timestamp'], errors='coerce')\n",
    "            dt_num = ts_num.diff().dropna()\n",
    "            dt_num = dt_num[dt_num > 0]\n",
    "            if len(dt_num) > 0:\n",
    "                fs_est = 1.0 / float(dt_num.median())\n",
    "                print('estimated_fs_from_numeric_timestamp:', fs_est)\n",
    "            else:\n",
    "                print('[warn] could not estimate fs from timestamp')\n",
    "\n",
    "if npy_files:\n",
    "    sample_npy = npy_files[0]\n",
    "    arr = np.load(sample_npy)\n",
    "    print('sample_npy:', sample_npy)\n",
    "    print('npy_shape:', arr.shape)\n",
    "    if arr.ndim == 2 and arr.shape[1] == 3:\n",
    "        print('[ok] npy shape is (T, 3)')\n",
    "    else:\n",
    "        print('[warn] expected npy shape (T, 3)')\n",
    "\n",
    "print('next: set data.fs in configs/swinmae_ssl_real.yaml to the real sampling rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import shlex\n",
    "\n",
    "\n",
    "def run(cmd):\n",
    "    cmd = [str(x) for x in cmd]\n",
    "    print('$', ' '.join(shlex.quote(x) for x in cmd))\n",
    "    proc = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "    )\n",
    "\n",
    "    assert proc.stdout is not None\n",
    "    for line in proc.stdout:\n",
    "        print(line, end='')\n",
    "\n",
    "    code = proc.wait()\n",
    "    if code != 0:\n",
    "        raise RuntimeError(f\"Command failed ({code}): {' '.join(cmd)}\")\n",
    "\n",
    "\n",
    "req = Path('requirements.txt')\n",
    "if not req.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"requirements.txt not found in cwd={Path.cwd()}. Run bootstrap cell first or fix repo path.\"\n",
    "    )\n",
    "\n",
    "run([sys.executable, '-m', 'pip', 'install', '-U', 'pip'])\n",
    "run([sys.executable, '-m', 'pip', 'install', '-r', str(req)])\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print('torch:', torch.__version__)\n",
    "print('cuda_available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda_device_count:', torch.cuda.device_count())\n",
    "    print('cuda_device_0:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "cfg = Path('configs/swinmae_ssl_real.yaml')\n",
    "if not cfg.exists():\n",
    "    print('[warn] real config not found, fallback to synthetic config')\n",
    "    cfg = Path('configs/swinmae_ssl.yaml')\n",
    "\n",
    "cmd = [sys.executable, '-m', 'trainers.train_swinmae_ssl', '--config', str(cfg)]\n",
    "print('$', ' '.join(cmd))\n",
    "result = subprocess.run(cmd)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(f\"Training failed with exit code {result.returncode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = Path('checkpoints/swinmae_ssl.pt')\n",
    "print('checkpoint_exists:', checkpoint_path.exists(), checkpoint_path)\n",
    "assert checkpoint_path.exists(), f'Missing checkpoint: {checkpoint_path}'\n",
    "print('checkpoint_size_bytes:', checkpoint_path.stat().st_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}